# -*- coding: utf-8 -*-
"""proposal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lgxjw3-R2vAH6y636loaV1ZX-yv-MmTA
"""

!pip install gradio
!pip install langchain_groq
!pip install JSON
!pip install groq
!pip install pdfplumber
!pip install pymupdf
import traceback
import pdfplumber
import gradio as gr
import json
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

# Step 1: Define the Prompt Template
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert in writing Request for Proposals (RFPs). "
                   "Based on the provided title and description, generate a professional RFP."),
        ("human", "Title: {title}\nDescription: {description}")
    ]
)

# Step 2: Set Up the LLM
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")  # Replace with your actual API key

# Step 3: Create the Chain
chain = prompt_template | llm

def format_rfp_html(rfp_content):
    html = f"""
    <h2>Introduction</h2>
    <p>{rfp_content.get('introduction', 'N/A')}</p>

    <h2>Scope of Work</h2>
    <p>{rfp_content.get('scope_of_work', 'N/A')}</p>

    <h2>Proposal Submission</h2>
    <p>{rfp_content.get('proposal_submission', 'N/A')}</p>

    <h2>Evaluation Criteria</h2>
    <p>{rfp_content.get('evaluation_criteria', 'N/A')}</p>

    <h2>Contract Terms</h2>
    <p>{rfp_content.get('contract_terms', 'N/A')}</p>
    """
    return html
def generate_rfp(title, description):
    try:
        # Format the prompt
        prompt_text = prompt_template.format_messages(title=title, description=description)

        # Generate the response
        response = llm(prompt_text)

        # Attempt to parse as JSON
        if isinstance(response.content, str):
            try:
                rfp_content = json.loads(response.content)  # Parse as JSON
                rfp_html = format_rfp_html(rfp_content)
                return rfp_html
            except json.JSONDecodeError:
                # Fall back to returning plain content if JSON parsing fails
                return response.content  # Plain text response
        else:
            return "Unexpected response format from the AI."

    except Exception as e:
        print("Detailed Error:", traceback.format_exc())  # Log for debugging
        return "An error occurred while generating the RFP. Please try again."






# Step 5: Create Gradio Interface
gr.Interface(
    fn=generate_rfp,
    inputs=[
        gr.Textbox(label="RFP Title"),
        gr.Textbox(label="RFP Description", lines=5)
    ],
    outputs=gr.Markdown(),  # Use Markdown for formatted output
    title="AI RFP Writer",
    description="Generate a professional RFP using AI."
).launch()

#INSTANT PINK TEAM DRAFT

# Step 1: Define the Prompt Template
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert in writing Request for Proposals (RFPs). "
                   "Based on the provided documents and draft outline, generate a professional RFP."),
        ("human", "Context from Documents:\n{documents_content}\n\nDraft Outline:\n{draft_outline}")
    ]
)

# Step 2: Set Up the LLM using ChatGroq
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_file):
    text = ""
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                extracted_text = page.extract_text()
                if extracted_text:  # Check if the page has text
                    text += extracted_text + "\n"
    except Exception as e:
        print(f"Error extracting text from PDF: {e}")
        text += f"Error extracting text from PDF: {e}\n"
    return text

# Function to generate RFP content using the AI model
def generate_rfp(draft_outline, uploaded_files):
    try:
        # Extract text from all uploaded PDF files
        documents_content = ""
        for pdf in uploaded_files:
            documents_content += extract_text_from_pdf(pdf.name) + "\n"  # Use pdf.name to get the file path

        # Create the prompt using the prompt template
        prompt_text = prompt_template.format_messages(documents_content=documents_content, draft_outline=draft_outline)

        # Generate the response using the ChatGroq model
        response = llm(prompt_text)

        # Check if response content is valid
        if isinstance(response.content, str):
            return response.content  # Return the generated RFP text
        else:
            return "Error: Unexpected response format. Please try again."

    except Exception as e:
        print(f"Error generating RFP: {e}")
        return f"Error generating RFP: {e}"


# Create the Gradio interface
interface = gr.Interface(
    fn=generate_rfp,
    inputs=[
        gr.Textbox(lines=10, placeholder="Enter the draft outline here...", label="Draft Outline"),
        gr.File(file_count="multiple", label="Upload PDF Documents")
    ],
    outputs="text",
    title="Instant Pink Team Draft",
    description="Upload your RFP documents and provide an outline. The AI will generate a new RFP draft by pulling relevant context from the uploaded documents."
)

# Launch Gradio Interface
interface.launch()

#PRECISION FOCUS


# Step 1: Set Up the LLM using ChatGroq
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")

# Step 1: Define the Prompt Template
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert in writing Precision Focus documents. "
            "Based on the provided documents and draft outline, generate a professional Precision Focus document.",
        ),
        ("human", "Context from Documents:\n{documents_content}\n\nDraft Outline:\n{draft_outline}"),
    ]
)

# Step 2: Set Up the LLM using ChatGroq
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_file):
    text = ""
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + "\n"
    except Exception as e:
        print(f"Error extracting text from {pdf_file}: {e}")
        traceback.print_exc()
    return text

# Function to generate Precision Focus content using the AI model
def generate_precision_focus(draft_outline, uploaded_files):
    try:
        # Extract text from all uploaded PDF files
        documents_content = ""
        for pdf in uploaded_files:
            documents_content += extract_text_from_pdf(pdf.name) + "\n"  # Use pdf.name to get the file path

        # Create the prompt using the prompt template
        prompt_text = prompt_template.format_messages(documents_content=documents_content, draft_outline=draft_outline)

         # Generate the response using the ChatGroq model
        response = llm(prompt_text)

        # Check if response content is valid
        if isinstance(response.content, str):
            return response.content  # Return the generated Precision Focus text
        else:
            return "Error: Unexpected response format. Please try again."

    except Exception as e:
        print(f"Error generating Precision Focus: {e}")
        return f"Error generating Precision Focus: {e}"

# Create the Gradio interface
interface = gr.Interface(
    fn=generate_precision_focus,
    inputs=[
        gr.Textbox(lines=10, placeholder="Enter the draft outline here...", label="Draft Outline"),
        gr.File(file_count="multiple", label="Upload PDF Documents")
    ],
    outputs="text",
    title="Instant Precision Focus",
    description="Upload your documents and provide an outline. The AI will generate a Precision Focus document by pulling relevant context from the uploaded documents."
)

# Launch Gradio Interface
interface.launch()

#Smart Bar
# Load the LLaMA model using ChatGroq
model = ChatGroq(model_name="llama3-8b-8192", api_key="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") # Replace with your actual API key

# Define the function to expand the text
def expand_text(text):
    prompt = f"Expand the following text: {text}"
    result = model.generate_text(prompt)  # Assuming your model has a 'generate_text' method
    return result

# Define the function to shorten the text
def shorten_text(text):
    prompt = f"Summarize the following text: {text}"
    result = model.generate_text(prompt)
    return result

# Define the function to correct grammar
def correct_grammar(text):
    prompt = f"Correct the grammar in the following text: {text}"
    result = model.generate_text(prompt)
    return result

# Define the function to convert text to bullet points
def bullet_points(text):
    prompt = f"Convert the following text into bullet points: {text}"
    result = model.generate_text(prompt)
    return result

# Define the Gradio interface
def smart_bar_interface(text, action):
    try:
        if action == "Expand":
            prompt = f"Expand the following text: {text}"
        elif action == "Shorten":
            prompt = f"Summarize the following text: {text}"
        elif action == "Correct Grammar":
            prompt = f"Correct the grammar in the following text: {text}"
        elif action == "Bullet Points":
            prompt = f"Convert the following text into bullet points: {text}"
        else:
            raise ValueError("Invalid action")

        # Update this part with the correct method from ChatGroq
        result = model.predict(prompt)  # or model.predict(prompt) or model.generate(prompt)
        return result


    except Exception as e:
        # Log the error for debugging
        print(f"Error in {action}: {e}")
        return f"Error while processing your request: {str(e)}"

# Define the Gradio interface elements
interface = gr.Interface(
    fn=smart_bar_interface,
    inputs=[gr.Textbox(label="Input Text"), gr.Radio(["Expand", "Shorten", "Correct Grammar", "Bullet Points"], label="Action")],
    outputs="text",
    title="Smart Bar",
)

# Launch the Gradio interface
interface.launch()

#PRECISION FOCUS


# Step 1: Set Up the LLM using ChatGroq
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")

# Step 1: Define the Prompt Template
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert in writing Precision Focus documents. "
            "Based on the provided documents and draft outline, generate a professional Precision Focus document.",
        ),
        ("human", "Context from Documents:\n{documents_content}\n\nDraft Outline:\n{draft_outline}"),
    ]
)

# Step 2: Set Up the LLM using ChatGroq
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_file):
    text = ""
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + "\n"
    except Exception as e:
        print(f"Error extracting text from {pdf_file}: {e}")
        traceback.print_exc()
    return text

# Function to generate Precision Focus content using the AI model
def generate_precision_focus(draft_outline, uploaded_files):
    try:
        # Extract text from all uploaded PDF files
        documents_content = ""
        for pdf in uploaded_files:
            documents_content += extract_text_from_pdf(pdf.name) + "\n"  # Use pdf.name to get the file path

        # Create the prompt using the prompt template
        prompt_text = prompt_template.format_messages(documents_content=documents_content, draft_outline=draft_outline)

         # Generate the response using the ChatGroq model
        response = llm(prompt_text)

        # Check if response content is valid
        if isinstance(response.content, str):
            return response.content  # Return the generated Precision Focus text
        else:
            return "Error: Unexpected response format. Please try again."

    except Exception as e:
        print(f"Error generating Precision Focus: {e}")
        return f"Error generating Precision Focus: {e}"

# Create the Gradio interface
interface = gr.Interface(
    fn=generate_precision_focus,
    inputs=[
        gr.Textbox(lines=10, placeholder="Enter the draft outline here...", label="Draft Outline"),
        gr.File(file_count="multiple", label="Upload PDF Documents")
    ],
    outputs="text",
    title="Instant Precision Focus",
    description="Upload your documents and provide an outline. The AI will generate a Precision Focus document by pulling relevant context from the uploaded documents."
)

# Launch Gradio Interface
interface.launch()



#ROUGE CHAT

# Function to dynamically call the appropriate text generation method
def call_model_for_text(prompt):
    try:
        # Try using the 'predict' method instead of 'generate'
        result = model.predict(prompt)
        return result
    except Exception as e:
        return f"Error during model generation: {str(e)}"


# Function to read and extract text from different file types
def extract_text_from_document(doc):
    try:
        # Handle text files (.txt)
        if doc.name.endswith('.txt'):
            return doc.decode('utf-8')  # Decode to get text content

        # Handle PDF files
        elif doc.name.endswith('.pdf'):
            pdf_reader = fitz.open(stream=doc, filetype="pdf")  # Open the PDF from the file object
            pdf_text = ""
            for page_num in range(len(pdf_reader)):
                page = pdf_reader.load_page(page_num)
                pdf_text += page.get_text()  # Extract text from each page
            return pdf_text

        else:
            return "Unsupported file format."

    except Exception as e:
        return f"Error processing document: {str(e)}"


# Function to generate a proposal based on input, compliance items, and documents
def generate_proposal(text, compliance_items, documents):
    try:
        # Prepare the input prompt based on user input
        prompt = f"Generate a proposal based on the following input:\n\nText: {text}\n\nCompliance Items: {', '.join(compliance_items)}"

        # Append document content to the prompt
        if documents:
            prompt += "\n\nDocuments: "
            for document in documents:
                doc_content = extract_text_from_document(document)
                prompt += f"\n\n{doc_content}"

        # Call the model to generate a proposal using the appropriate method
        result = call_model_for_text(prompt)
        return result
    except Exception as e:
        return f"Error while generating proposal: {str(e)}"

# Function to expand text
def expand_text(text):
    try:
        prompt = f"Expand the following text: {text}"
        result = call_model_for_text(prompt)
        return result
    except Exception as e:
        return f"Error while expanding text: {str(e)}"



# Define the Gradio interface for Rogue Chat
def rogue_chat_interface(text, action, compliance_items, documents):
    try:
        if action == "Generate Proposal":
            return generate_proposal(text, compliance_items, documents)
        elif action == "Expand":
            return expand_text(text)
        else:
            return "Invalid action selected."
    except Exception as e:
        return f"Error in action '{action}': {str(e)}"

# Define the Gradio interface elements
interface = gr.Interface(
    fn=rogue_chat_interface,
    inputs=[
        gr.Textbox(label="Input Text", placeholder="Enter the proposal or text to process"),  # Text input
        gr.Radio(["Generate Proposal", "Expand", ], label="Action"),  # Action to take
        gr.CheckboxGroup(choices=["GDPR compliance", "HIPAA compliance", "SOX compliance"], label="Compliance Items"),  # Compliance options
        gr.File(label="Upload Documents", file_count="multiple", file_types=[".txt", ".pdf"])  # File upload for documents
    ],
    outputs="text",  # Output is the processed text
    title="Rogue Chat - Proposal Generation and Text Operations",
    description="A specialized chat interface for generating proposals with compliance checks, document handling, and more."
)

# Launch the Gradio interface
interface.launch()

# Commented out IPython magic to ensure Python compatibility.
!git config --global user.name "MuskaanShrimali"
!git config --global user.email "muskaanshrimali1@gmail.com"
!git clone https://github.com/ MuskaanShrimali/https://github.com/MuskaanShrimali/Contract/blob/main/README.md
# %cd  Contract
!cp /content/sample_code.py /content/your-repo/https://github.com/MuskaanShrimali/Contract/blob/main/README.md
!git add proposal.ipynb
!git commit -m "Updated proposal notebook"
!git push https://your-username MuskaanShrimali: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/MuskaanShrimali/https://github.com/MuskaanShrimali/Contract/blob/main/README.md
!git clone https://<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>@github.com/username/repository.git

!pip install gradio
!pip install groq
!pip install groq-client
!pip install pdfplumber
!pip install pymupdf
!pip install langchain_core
!pip install langchain_groq
import gradio as gr
import torch
import pdfplumber
import re
import pandas as pd
from langchain_groq import ChatGroq

def extract_requirements_from_pdf(pdf_path):
    compliance_keywords = r'\b(shall|must|will|should|require|mandatory)\b'
    requirements = []

    try:
        with pdfplumber.open(pdf_path) as pdf:
            if not pdf.pages:
                raise ValueError("No pages found in the PDF document.")
            for page_number, page in enumerate(pdf.pages):
                text = page.extract_text()
                if not text:
                    continue  # Skip empty pages
                sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
                for sentence in sentences:
                    if re.search(compliance_keywords, sentence, re.IGNORECASE):
                        requirements.append({
                            "Requirement": sentence.strip(),
                            "Section Reference": f"Page {page_number + 1}"
                        })
    except Exception as e:
        return f"Error processing PDF: {e}"

    return requirements

def generate_compliance_matrix(pdf_path):
    requirements = extract_requirements_from_pdf(pdf_path)

    if not requirements:
        return "No requirements found in the uploaded document."

    df = pd.DataFrame(requirements)
    df["Compliant"] = ""  # Placeholder for compliance status (Yes/No)

    return df

def load_groq_model():
    try:
        model_name = "llama3-8b-8192"  # Adjust with Groq's model name
        model = ChatGroq(model_name=model_name, api_key="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")
        return model
    except Exception as e:
        return None, f"Error loading Groq model: {e}"

def generate_compliance_content(requirement_text):
    model, error = load_groq_model()
    if error:
        return error

    prompt = f"Compliance Response: {requirement_text}"
    response = model.generate_text(prompt)  # Assuming your model has a 'generate_text' method
    return response

def compliance_matrix_interface(pdf_file):
    pdf_path = pdf_file.name  # This may need to be adjusted based on how Gradio handles file uploads
    compliance_matrix = generate_compliance_matrix(pdf_path)

    if isinstance(compliance_matrix, str):
        return compliance_matrix  # Error message if no requirements are found or extraction fails

    for idx, row in compliance_matrix.iterrows():
        requirement = row['Requirement']
        generated_content = generate_compliance_content(requirement)
        compliance_matrix.at[idx, 'Generated Content'] = generated_content

    return compliance_matrix.to_html(index=False)

# Gradio Interface
interface = gr.Interface(
    fn=compliance_matrix_interface,
    inputs=gr.File(label="Upload RFP PDF"),
    outputs=gr.HTML(label="Compliance Matrix"),
    title="Compliance Matrix Generator",
    description="Upload an RFP PDF and generate a compliance matrix with suggested responses."
)

# Launch the Gradio interface
interface.launch()

import traceback
import pdfplumber
import gradio as gr
import json
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

# Step 1: Define the Prompt Template
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert in writing Request for Proposals (RFPs). "
                   "Based on the provided title and description, generate a professional RFP."),
        ("human", "Title: {title}\nDescription: {description}")
    ]
)

# Step 2: Set Up the LLM
llm = ChatGroq(model_name="llama3-8b-8192", api_key="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")  # Replace with your actual API key

# Step 3: Create the Chain
chain = prompt_template | llm

def format_rfp_html(rfp_content):
    html = f"""
    <h2>Introduction</h2>
    <p>{rfp_content.get('introduction', 'N/A')}</p>

    <h2>Scope of Work</h2>
    <p>{rfp_content.get('scope_of_work', 'N/A')}</p>

    <h2>Proposal Submission</h2>
    <p>{rfp_content.get('proposal_submission', 'N/A')}</p>

    <h2>Evaluation Criteria</h2>
    <p>{rfp_content.get('evaluation_criteria', 'N/A')}</p>

    <h2>Contract Terms</h2>
    <p>{rfp_content.get('contract_terms', 'N/A')}</p>
    """
    return html
def generate_rfp(title, description):
    try:
        # Format the prompt
        prompt_text = prompt_template.format_messages(title=title, description=description)

        # Generate the response
        response = llm(prompt_text)

        # Attempt to parse as JSON
        if isinstance(response.content, str):
            try:
                rfp_content = json.loads(response.content)  # Parse as JSON
                rfp_html = format_rfp_html(rfp_content)
                return rfp_html
            except json.JSONDecodeError:
                # Fall back to returning plain content if JSON parsing fails
                return response.content  # Plain text response
        else:
            return "Unexpected response format from the AI."

    except Exception as e:
        print("Detailed Error:", traceback.format_exc())  # Log for debugging
        return "An error occurred while generating the RFP. Please try again."






# Step 5: Create Gradio Interface
gr.Interface(
    fn=generate_rfp,
    inputs=[
        gr.Textbox(label="RFP Title"),
        gr.Textbox(label="RFP Description", lines=5)
    ],
    outputs=gr.Markdown(),  # Use Markdown for formatted output
    title="AI RFP Writer",
    description="Generate a professional RFP using AI."
).launch()

!pip install gradio
!pip install groq
!pip install groq-client
!pip install pdfplumber
!pip install pymupdf
!pip install langchain_core
!pip install langchain_groq
import gradio as gr
import pdfplumber
import os
import json
from langchain_groq import ChatGroq

# Load the Groq model
llm = ChatGroq(model_name="llama3-8b-8192",api_key="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")

# Directory for saving templates
TEMPLATE_DIR = "rogue_templates"
os.makedirs(TEMPLATE_DIR, exist_ok=True)

# Step 1: Extract text from a PDF
def extract_text_from_pdf(pdf_file):
    text = ""
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Step 2: Save a template
def save_template(template_name, sections):
    template_path = os.path.join(TEMPLATE_DIR, f"{template_name}.json")
    with open(template_path, "w") as file:
        json.dump({"template_name": template_name, "sections": sections}, file)
    print(f"Template '{template_name}' saved successfully!")

# Step 3: Load a template
def load_template(template_name):
    template_path = os.path.join(TEMPLATE_DIR, f"{template_name}.json")
    if os.path.exists(template_path):
        with open(template_path, "r") as file:
            return json.load(file)
    else:
        return None

# Step 4: Pre-save standard templates
def pre_save_templates():
    templates = [
        {
            "template_name": "RFI",
            "sections": [
                {"title": "Introduction", "content": ""},
                {"title": "Project Scope", "content": ""},
                {"title": "Deliverables", "content": ""},
                {"title": "Timeline", "content": ""},
                {"title": "Evaluation Criteria", "content": ""},
                {"title": "Proposal Submission", "content": ""}
            ]
        },
        {
            "template_name": "RFQ",
            "sections": [
                {"title": "Introduction", "content": ""},
                {"title": "Requirements", "content": ""},
                {"title": "Pricing", "content": ""},
                {"title": "Submission Instructions", "content": ""}
            ]
        },
        # Add more templates as required
         {
            "template_name": "Sources Sought",
            "sections": [
                {"title": "Introduction", "content": ""},
                {"title": "Requirements", "content": ""},
                {"title": "Capability Statement", "content": ""},
                {"title": "Submission Details", "content": ""}
            ]
        },

        {
            "template_name": "SOW",
            "sections": [
                {"title": "Purpose", "content": ""},
                {"title": "Scope", "content": ""},
                {"title": "Objectives", "content": ""},
                {"title": "Requirements", "content": ""},
                {"title": "Deliverables", "content": ""},
                {"title": "Timeline", "content": ""}
            ]
        },
        {
            "template_name": "RFP",
            "sections": [
                {"title": "Introduction", "content": ""},
                {"title": "Scope of Work", "content": ""},
                {"title": "Proposal Submission", "content": ""},
                {"title": "Evaluation Criteria", "content": ""},
                {"title": "Contract Terms", "content": ""}
            ]
        },
        {
            "template_name": "Capability Statement",
            "sections": [
                {"title": "Company Overview", "content": ""},
                {"title": "Core Competencies", "content": ""},
                {"title": "Past Performance", "content": ""},
                {"title": "Differentiators", "content": ""},
                {"title": "Corporate Data", "content": ""}
            ]
        },
        {
            "template_name": "Contract Award Notice",
            "sections": [
                {"title": "Contract Award Information", "content": ""},
                {"title": "Contractor Information", "content": ""},
                {"title": "Contract Value", "content": ""},
                {"title": "Contract Performance Period", "content": ""},
                {"title": "Description of Work", "content": ""}
            ]
        }

    ]
    for template in templates:
        save_template(template["template_name"], template["sections"])

# Ensure templates are pre-saved
pre_save_templates()

# Step 5: Generate RFP content based on a template
def generate_rfp_with_template(template_name, uploaded_files):
    # Load the selected template
    template = load_template(template_name)
    if not template:
        return f"Template '{template_name}' not found."

    # Extract text from uploaded files
    documents_content = ""
    for pdf in uploaded_files:
        documents_content += extract_text_from_pdf(pdf) + "\n"

    # Fill in the content for each section
    filled_template = f"RFP: {template_name}\n\n"
    for section in template["sections"]:
        prompt = f"Using the following documents, generate the content for the {section['title']} section:\n\n{documents_content}"
        generated_content = llm.generate(prompt, max_tokens=1024)
        filled_template += f"{section['title']}\n{generated_content['text']}\n\n"

    return filled_template

# Step 6: Precision Focus - Insert Content Dynamically
def precision_focus(draft_outline, uploaded_files):
    # Extract text from uploaded files
    documents_content = "\n".join([extract_text_from_pdf(pdf) for pdf in uploaded_files])

    # Replace "@" with relevant content
    if "@" in draft_outline:
        keyword = "@"  # Define the focus symbol
        relevant_content = documents_content[:512]  # Simplified for demo purposes
        draft_outline = draft_outline.replace(keyword, relevant_content, 1)

    return draft_outline

# Gradio Interface for Precision Focus
def precision_focus_interface(draft_outline, uploaded_files):
    return precision_focus(draft_outline, uploaded_files)

# Step 7: Gradio Interface for RFP Generation
interface = gr.Interface(
    fn=precision_focus_interface,
    inputs=[
        gr.Textbox(lines=10, placeholder="Enter draft outline, use '@' to focus on uploaded content...", label="Draft Outline"),
        gr.File(file_count="multiple", type="filepath", label="Upload PDF Documents")
    ],
    outputs="text",
    title="Precision Focus RFP Generator with Groq",
    description="Upload your RFP documents and use '@' in your outline to dynamically insert content from uploaded files."
)

# Launch the Gradio interface
interface.launch()